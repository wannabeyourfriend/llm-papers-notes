# DEEPSEEK-V3: Open MoE Frontier Model

**arXiv**: [2512.02556](https://arxiv.org/abs/2512.02556) | **Date**: December 2024 | **Team**: DeepSeek

[üìÑ PDF](https://arxiv.org/pdf/2512.02556.pdf) | [ü§ó Model](https://github.com/deepseek-ai/DeepSeek-V3) | [üíª Code](https://github.com/deepseek-ai/DeepSeek-V3)

---

## üéØ One-Line Summary

671B-parameter MoE model (37B active) achieving frontier-level performance competitive with GPT-4o/Claude-3.5 at **$5.578M training cost** (10-20√ó lower) through FP8 training, expert parallelism, and multi-token prediction.

## üìä Key Results

| Benchmark | Score | Rank |
|-----------|-------|------|
| MMLU | 88.5 | #3 (vs GPT-4o: 88.7) |
| MMLU-Pro | 78.0 | #3 (vs GPT-4o: 80.4) |
| GPQA Diamond | 68.0 | #2 (tied with Gemini-U) |
| Codeforces | 2299 | #2 (vs Claude-3.5: 2322) |
| LiveCodeBench | 51.2 | #3 |
| SWE-Bench Verified | 48.8 | #3 (vs Claude-3.5: 49.0) |
| Math-500 | 92.0 | #2 |
| C-Eval | 90.2 | #2 |
| CMMLU | 90.8 | #2 |
| HumanEval (0-shot) | 92.8 | #4 |

## üèóÔ∏è Architecture

```
Total Parameters:        671B (186B attention + 385B FFN + 100B routing)
Active Parameters:       37B per token (5.5% of total)
Experts:                 256 total, 8 active per token
Top-K Routing:           k=8
Architecture:            Multi-head Latent Attention (MLA) + DeepSeekMoE
Hidden Size:             7,168
Attention Heads:         128 (key-value heads)
Layers:                  61
Context Length:          128K tokens
Vocabulary:             102,400 tokens
Training Tokens:         14.8T (pre-training)
```

## üî¨ Core Techniques

### 1. Multi-Head Latent Attention (MLA)

**Problem**: Standard KV caching requires enormous memory for long contexts.

**Solution**: Compress key-value pairs into latent vectors using low-rank key-value projection.

**Results**: Enables 128K context with minimal memory overhead; reduces KV cache size without performance loss.

### 2. Multi-Token Prediction (MTP)

**Problem**: Next-token prediction is slow during inference.

**Solution**: Train model to predict multiple future tokens (k=5) per forward pass using auxiliary prediction heads.

**Results**: 1.5√ó inference speedup in draft phase; better generalization without primary task degradation.

### 3. DualPipe Pipeline Parallelism

**Problem**: Pipeline bubbles waste computation time.

**Solution**: Overlap forward/backward passes, all-gather, and all-reduce operations across pipeline stages.

**Results**: 40% higher communication-computation overlap; achieves 3.7M tokens/sec throughput on 2048 H100 GPUs.

### 4. Expert Parallelism with DeepSeekMoE

**Problem**: Load imbalance in MoE causes expert underutilization.

**Solution**: (1) Separated experts (1 shared + n-routed per layer), (2) Affinity-based load balancing loss, (3) Expert parallelism routing.

**Results**: 97.2% expert utilization; faster convergence; enables scaling to 256 experts.

### 5. FP8 Training with Loss Scaling

**Problem**: FP8 overflow risks training instability.

**Solution**: Per-tensor dynamic quantization + gradient clipping with 8-bit exponent.

**Results**: Stable training at 1.2M tokens/sec; no performance degradation vs BF16 baseline.

## üìà Training

**Data**:
- Pre-training: 14.8T tokens (8.1T high-quality + 6.7T auxiliary)
- Data mix: 80% English, 10% Chinese, 10% code/math
- Context: 8K (pre-training) ‚Üí 128K (post-training)

**Optimization**:
- Optimizer: AdamW (Œ≤1=0.9, Œ≤2=0.95)
- Learning rate: 4.2e-4 peak, 3000B token cosine decay
- Batch size: 7.6M tokens (4608 sequences √ó 2048 GPUs)
- Sequence length: 8K tokens
- Precision: FP8 (forward) + BF16 (master weights)

**Infrastructure**:
- Hardware: 2048 NVIDIA H100 GPUs (512 nodes √ó 8 GPUs)
- Network: InfiniBand QDR (400 Gbps)
- Training time: 2.5 months (wall-clock)
- Throughput: 3.7M tokens/sec peak
- Total compute cost: $5.578M (at $2.67/H100-hour)
- Framework: DeepSpeed + Megatron + DeepSeek-FL

## ‚ö†Ô∏è Limitations

1. Math reasoning: Underperforms on complex proofs (Math-500: 92.0 vs GPT-4o: 94.0)
2. Instruction following: IF-Eval (86.5) lags Claude-3.5 (89.6)
3. Code debugging: Lower performance on bug-fixing vs Claude-3.5
4. Non-English languages: Focus on Chinese/English; others underperform
5. Long-context retrieval: 128K context but imperfect needle-in-haystack at full length

## üîó Links

- **Paper**: https://arxiv.org/abs/2512.02556
- **PDF**: https://arxiv.org/pdf/2512.02556.pdf
- **Code**: https://github.com/deepseek-ai/DeepSeek-V3
- **Model**: https://github.com/deepseek-ai/DeepSeek-V3 (weights & API)
- **Project**: https://www.deepseek.com/

---

**Tags**: #MoE #FP8-training #expert-parallelism #cost-efficiency #open-weights
